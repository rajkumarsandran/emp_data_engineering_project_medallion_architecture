{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d376168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = SparkSession.builder.config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e59ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"create schema if not exists bronze\")\n",
    "spark.sql(\"create schema if not exists silver\")\n",
    "spark.sql(\"create schema if not exists gold\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists bronze.processed_files (load_date date, bronze_table_name string, source_file_name string, status string) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/bronze/processed_files'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists bronze.emp (empno integer,ename string,job string,mgr integer,hiredate date,sal double,comm double,deptno integer,source_file_name string,load_date date) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/bronze/emp'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists silver.emp (emp_sk bigint,emp_hash string,empno integer,ename string,job string,mgr integer,hiredate date,sal double,comm double,deptno integer,load_date date, effective_start_date date, effective_end_date date, is_active boolean) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/silver/emp'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists gold.dim_emp (empno integer,ename string,job string,mgr integer,hiredate date,sal double,comm double,deptno integer,emp_hash string,last_updated_date date) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/gold/dim_emp'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists gold.fact_emp_headcount (deptno integer,total_emps long, as_of_date date) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/gold/fact_emp_headcount'\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table if not exists gold.fact_emp_salary (deptno integer,total_sal double, as_of_date date) \n",
    "using delta\n",
    "location 'file:/C:/Apps/spark-warehouse/gold/fact_emp_salary'\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a611e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23\n"
     ]
    }
   ],
   "source": [
    "load_date = spark.range(1).select(f.date_add(f.current_date(),-2).alias('load_date')).collect()[0][0]\n",
    "print(load_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc7687",
   "metadata": {},
   "source": [
    "## Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fcbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = spark.table('bronze.processed_files')\n",
    "\n",
    "emp_schema = \"empno integer,ename string,job string,mgr integer,hiredate date,sal double,comm double,deptno integer\"\n",
    "\n",
    "emp = spark.read.load(format='csv', path=\"emp\", header=True, schema=emp_schema)\n",
    "\n",
    "emp = emp.withColumn('source_file_name',f.input_file_name())\n",
    "\n",
    "unprocessed_data = emp.alias('e').join(processed_files.alias('p'), 'source_file_name', 'left_anti')\n",
    "\n",
    "bronze_emp = unprocessed_data.select(f.col('e.*')).withColumn('load_date',f.lit(load_date))\n",
    "\n",
    "try:\n",
    "    bronze_emp.write.saveAsTable(name='bronze.emp', format='delta', mode='append')\n",
    "    status = 'PROCESSED'\n",
    "except Exception as e:\n",
    "    status = 'FAILED'\n",
    "\n",
    "status_df = bronze_emp.select(f.col('source_file_name'), f.col('load_date')).distinct()\\\n",
    "    .withColumn('bronze_table_name',f.lit('emp')).withColumn('status', f.lit(status))\n",
    "\n",
    "status_df.write.saveAsTable(name='bronze.processed_files', format='delta', mode='append')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea969a1",
   "metadata": {},
   "source": [
    "## Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3052b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bronze_emp = spark.table('bronze.emp')\n",
    "\n",
    "target_silver_emp = spark.table('silver.emp')\n",
    "\n",
    "max_sk = target_silver_emp.select(f.coalesce(f.max(f.col('emp_sk')),f.lit(0))).collect()[0][0]\n",
    "\n",
    "hash_cols = ['empno','ename','job','mgr','hiredate','sal','comm','deptno']\n",
    "\n",
    "required_silver_cols = ['empno','ename','job','mgr','hiredate','sal','comm','deptno','load_date']\n",
    "\n",
    "source_silver_emp = bronze_emp.where(f.col('load_date')==load_date)\\\n",
    "    .withColumn('rnk', f.row_number().over(Window.partitionBy(f.col('empno')).orderBy(f.col('load_date').desc())))\\\n",
    "    .where(f.col('rnk')==1)\\\n",
    "    .select(*[f.col(c) for c in required_silver_cols])\\\n",
    "    .withColumn('effective_start_date',f.lit(load_date))\\\n",
    "    .withColumn('effective_end_date',f.lit(None).cast('date'))\\\n",
    "    .withColumn('is_active', f.lit(True))\\\n",
    "    .withColumn('emp_hash', f.sha2(f.concat_ws('||',*[f.coalesce(f.col(c).cast('string'), f.lit('NULL')) for c in hash_cols]), 256))\n",
    "\n",
    "\n",
    "update_rows = source_silver_emp.alias('s')\\\n",
    "    .join(\n",
    "        target_silver_emp.where(f.col('is_active')==True).alias('t'), \n",
    "        [f.col('s.empno')==f.col('t.empno'), f.col('s.emp_hash')!=f.col('t.emp_hash')],\n",
    "        'left_semi'\n",
    "    ).withColumn('action', f.lit('update')).withColumn('emp_sk', f.lit(None).cast('bigint'))\n",
    "\n",
    "insert_rows = source_silver_emp.alias('s')\\\n",
    "    .join(\n",
    "        target_silver_emp.where(f.col('is_active')==True).alias('t'), \n",
    "        'emp_hash',\n",
    "        'left_anti'\n",
    "    ).withColumn('action', f.lit('insert')).withColumn('emp_sk', f.row_number().over(Window.orderBy(f.col('empno')))+max_sk)\n",
    "\n",
    "new_source_silver_emp = update_rows.unionByName(insert_rows)\n",
    "\n",
    "\n",
    "DeltaTable.forName(spark,'silver.emp').alias('t').merge(\n",
    "    source=new_source_silver_emp.alias('s'),\n",
    "    condition=\"s.empno=t.empno and t.is_active=true and s.action='update'\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"effective_end_date\": f.date_add(f.lit(load_date),-1),\n",
    "        \"is_active\": \"false\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"empno\": \"s.empno\",\n",
    "        \"ename\": \"s.ename\",\n",
    "        \"job\": \"s.job\",\n",
    "        \"mgr\": \"s.mgr\",\n",
    "        \"hiredate\": \"s.hiredate\",\n",
    "        \"sal\": \"s.sal\",\n",
    "        \"comm\": \"s.comm\",\n",
    "        \"deptno\": \"s.deptno\",\n",
    "        \"load_date\": \"s.load_date\",\n",
    "        \"effective_start_date\": \"s.effective_start_date\",\n",
    "        \"effective_end_date\": \"s.effective_end_date\",\n",
    "        \"is_active\": \"s.is_active\",\n",
    "        \"emp_sk\": \"s.emp_sk\",\n",
    "        \"emp_hash\": \"s.emp_hash\"\n",
    "    }\n",
    ").execute().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a380c1",
   "metadata": {},
   "source": [
    "## Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "018cb8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "silver_emp = spark.table('silver.emp')\n",
    "\n",
    "hash_cols = ['empno','ename','job','mgr','hiredate','sal','comm','deptno']\n",
    "\n",
    "dim_emp_required_cols = ['empno','ename','job','mgr','hiredate','sal','comm','deptno','emp_hash','last_updated_date']\n",
    "\n",
    "source_dim_emp = silver_emp.where((f.col('is_active')==True) & (f.col('load_date')==load_date))\\\n",
    "    .withColumn('last_updated_date',f.col('load_date'))\\\n",
    "    .withColumn('emp_hash', f.sha2(f.concat_ws('||',*[f.coalesce(f.col(c).cast('string'), f.lit('NULL')) for c in hash_cols]), 256))\\\n",
    "    .select(*[f.col(c) for c in dim_emp_required_cols])\n",
    "\n",
    "DeltaTable.forName(spark,'gold.dim_emp').alias('t').merge(\n",
    "    source=source_dim_emp.alias('s'),\n",
    "    condition=\"s.empno=t.empno\"\n",
    ").whenMatchedUpdateAll(condition=\"s.emp_hash!=t.emp_hash\").whenNotMatchedInsertAll().execute().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4b30303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_emp = spark.table('gold.dim_emp')\n",
    "\n",
    "fact_emp_headcount = dim_emp.groupBy(f.col('deptno')).agg(f.count('*').alias('total_emps'))\\\n",
    "    .withColumn('as_of_date',f.lit(load_date))\n",
    "\n",
    "DeltaTable.forName(spark, 'gold.fact_emp_headcount').alias('t').merge(\n",
    "    source=fact_emp_headcount.alias('s'),\n",
    "    condition=\"s.as_of_date=t.as_of_date and s.deptno=t.deptno\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"s.total_emps<>t.total_emps\",\n",
    "    set={\n",
    "        \"total_emps\": \"s.total_emps\"\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46eac9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_emp = spark.table('gold.dim_emp')\n",
    "\n",
    "fact_emp_salary = dim_emp.groupBy(f.col('deptno')).agg(f.sum(f.col('sal')).alias('total_sal'))\\\n",
    "    .withColumn('as_of_date',f.lit(load_date))\n",
    "\n",
    "\n",
    "DeltaTable.forName(spark, 'gold.fact_emp_salary').alias('t').merge(\n",
    "    source=fact_emp_salary.alias('s'),\n",
    "    condition=\"s.as_of_date=t.as_of_date and s.deptno=t.deptno\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"s.total_sal<>t.total_sal\",\n",
    "    set={\n",
    "        \"total_sal\": \"s.total_sal\"\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select * from gold.fact_emp_salary\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(sql).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
